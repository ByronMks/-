{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4287eb01",
   "metadata": {},
   "source": [
    "## Βήμα 1 - Συλλογή Εγγράφων\n",
    "\n",
    "Για να συλλέξουμε τα έγγραφα που χρειαζόμαστε δημιουργούμε έναν web crawler χρησιμοποιώντας το πακέτο BeautifulSoup της python. Το αρχείο που περιέχει τον κώδικα του web crawler είναι το wiki_crawler.py. Επιλέγουμε 10  κείμενα από τη wikipedia που είναι σχετικά με τις θεματικές του μαθήματος, έτσι θα έχουμε κατάλληλο όγκο πληροφορίας για να αναλύσουμε και να διαχειριστούμε με τον οποίο μπορούμε να αναδείξουμε αποτελεσματικά τις λειτουργίες της μηχανής αναζήτησης που θέλουμε να υλοποιήσουμε, χωρίς όμως να είναι υπερβολικός."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c645b29",
   "metadata": {},
   "source": [
    "Αρχικά  κάνουμε import τις απαραίτητες βιβλιοθήκες"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e9c244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895d3c5",
   "metadata": {},
   "source": [
    "Στη συνέχεια αρχικοποιούμε το URL από το οποίο θα γίνει υ συλλογή των εγγράφων, στην προκειμένη περίπτωση απο τη wikipedia, και τις θεματικές των άρθρων που θα συλλέξουμε."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20a6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://en.wikipedia.org/wiki/\"\n",
    "TOPICS = [\"Information_retrieval\", \"Natural_language_processing\", \"Artificial_intelligence\",\"Data_mining\", \"Machine_learning\", \"Deep_learning\", \"Computer_vision\", \"Neural_network\", \"Big_data\", \"Data_science\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4869d3",
   "metadata": {},
   "source": [
    "Ορίζουμε τη συνάρτηση scrape_wikipedia_articles, η οποία ψάχνει για τα άρθρα στο URL που έχουμε ήδη ορίσει και κάνει τις απράιτητες τροποποιήσεις έτσι ώστε τα κείμενα που συλλέχθηκαν να είναι έτοιμα για αποθήκευση.\n",
    "Η συνάρτηση δέχεται ως παράμετρο την λίστα με τα τις θεματικές και επιστρέφει μια λίστα από έγγραφα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2543ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia_articles(topics):\n",
    "    articles = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        url = BASE_URL + topic\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Τίτλος άρθρου\n",
    "            title = soup.find('h1').text\n",
    "            \n",
    "            # Περιεχόμενο άρθρου (μόνο από την κύρια παράγραφο)\n",
    "            paragraphs = soup.find_all('p')\n",
    "            content = ' '.join([para.text for para in paragraphs if para.text.strip()])\n",
    "            \n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}\")\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e1b4e",
   "metadata": {},
   "source": [
    "Στη συνέχεια εκτελούμε τη συνάρτηση"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e342fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Εκτέλεση\n",
    "articles = scrape_wikipedia_articles(TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c39308",
   "metadata": {},
   "source": [
    "Και αποθηκεύουμε τα λίστα με τα έγγραφα που συλλέχθηκαν στο αρχέιο wikipedia_articles.json εμφανίζοντας μήνυμα επιτυχόυς ολοκλήρωσης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ededd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 articles saved in 'wikipedia_articles.json'.\n"
     ]
    }
   ],
   "source": [
    "# Αποθήκευση σε JSON\n",
    "with open(\"wikipedia_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"{len(articles)} articles saved in 'wikipedia_articles.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b039cc4",
   "metadata": {},
   "source": [
    "## Βήμα 2 - Προεπεξεργασία κειμένου\n",
    "\n",
    "Για να καθαρίσουμε το κείμενο που έχουμε συλλέξει θα εφαρμόσουμε της παρακάτω τεχνικές με την εξής σειρά:\n",
    "•\tLowercasing\n",
    "•\tΑφαίρεση ειδικών χαρακτήρων\n",
    "•\tTokenization\n",
    "•\tStop-Word Removal\n",
    "•\tLemmatization\n",
    "Ο κώδικας περιέχεται στο αρχέιο data_cleaner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335fe28b",
   "metadata": {},
   "source": [
    "Κάνουμε import της απραίτητες βιβλιοθήκες και κατεβάζουμε τους απαραίτητους πότους απο το NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "078ea8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mak14\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mak14\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mak14\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mak14\\anaconda3\\lib\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "\n",
    "# Κατεβάζουμε τους απαραίτητους πόρους από το NLTK\n",
    "download('punkt')  # Για tokenization\n",
    "download('stopwords')  # Για stop-word removal\n",
    "download('wordnet')  # Για lemmatization\n",
    "download('omw-1.4')  # Βοηθητικά δεδομένα για lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc7845",
   "metadata": {},
   "source": [
    "Κάνουμε τις απαράιτητες αρχικοποιήσεις και ανοίγουμε και διαβάζουμε το αρχείο με τα έγγραφα από το προηγούμενο βήμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a16fcda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αρχικοποιήσεις\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Διαβάζουμε το dataset\n",
    "with open(\"wikipedia_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a6319",
   "metadata": {},
   "source": [
    "Ορίζουμε τη συνάρτηση preprocess_text, η οποία κάνει όλα τα βήματα που αναφέραμε παραπάνω για τον καθαρισμό του κειμένου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a152346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Συνάρτηση για προεπεξεργασία\n",
    "def preprocess_text(text):\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Αφαίρεση ειδικών χαρακτήρων (κρατάμε μόνο γράμματα και κενά)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Stop-word removal\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Επιστροφή των καθαρισμένων tokens ως ενιαίο κείμενο\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ef48a",
   "metadata": {},
   "source": [
    "Επεξεργαζόμστε τα άρθρα έτσι ώστε να είναι έτοιμα για αποθύκευση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b7e949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Επεξεργασία των άρθρων\n",
    "processed_articles = []\n",
    "for article in articles:\n",
    "    cleaned_content = preprocess_text(article[\"content\"])\n",
    "    processed_articles.append({\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": article[\"url\"],\n",
    "        \"cleaned_content\": cleaned_content\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a03a6",
   "metadata": {},
   "source": [
    "Και τελικά τα αποθηκεύουμε στο αρχείο processed_wikipedia_articles.json και εμφανίζουμε ανάλογο μήνυμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9829d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed! Cleaned data saved in 'processed_wikipedia_articles.json'.\n"
     ]
    }
   ],
   "source": [
    "# Αποθήκευση του \"καθαρισμένου\" dataset\n",
    "with open(\"processed_wikipedia_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_articles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Preprocessing completed! Cleaned data saved in 'processed_wikipedia_articles.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd42db",
   "metadata": {},
   "source": [
    "## Βήμα 3 - Ευρετήριο\n",
    "\n",
    "Για να υλοποιήσουμε το ανεστραμμένο ευρετήριο χρησιμοποιούμε το καθαρισμένο κείμενο από το προηγούμενο βήμα και δημιουργούμε μια λίστα λέξεων (tokens) η οποία με τη χρήση μιας δομής λεξικού defaultdict τα αποθηκεύει κατάλληλα και το δημιουργεί. Ο κώδικας περιέχεται στο αρχείο inverted_index.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a4b73",
   "metadata": {},
   "source": [
    "Αρχικά κάνουμε import τις απραίτητες βιβλιοθήκες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "000a22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea901dd",
   "metadata": {},
   "source": [
    "Στη συνέχεια φορτώνουμε το καθαρισμένο κείμενο απο το αρχείο που δημιουργήσαμε στο πορηγούμενω βήμα και ορίζουμε το ανεστραμένο ευρετήριο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a8f0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Φόρτωση των καθαρισμένων άρθρων από το προηγούμενο βήμα\n",
    "with open(\"processed_wikipedia_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "# Δημιουργία ανεστραμμένου ευρετηρίου\n",
    "inverted_index = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e6fb5",
   "metadata": {},
   "source": [
    "Κάνουμε τις κατάλληλες τροποποίησεις στο κείμενο για να κατασκευάζσουμε σωστά το ανεστραμένο ευρετήριο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "052b658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Κατασκευή του ανεστραμμένου ευρετηρίου\n",
    "for doc_id, article in enumerate(articles):\n",
    "    # Διαχωρισμός των λέξεων από το καθαρισμένο περιεχόμενο\n",
    "    tokens = article[\"cleaned_content\"].split()\n",
    "    # Για κάθε μοναδική λέξη στο έγγραφο\n",
    "    for token in set(tokens):  # Χρησιμοποιούμε set για να μην αποθηκεύσουμε διπλές εμφανίσεις\n",
    "        inverted_index[token].append(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f392c1a",
   "metadata": {},
   "source": [
    "Αποθηκέυουμε το ευρετήριο στο αρχείο inverted_index.py και εμφανίζουμε ανάλογο μήνυμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e666437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index created and save in 'inverted_index.json'.\n"
     ]
    }
   ],
   "source": [
    "# Αποθήκευση του ανεστραμμένου ευρετηρίου\n",
    "with open(\"inverted_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inverted_index, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Inverted index created and save in 'inverted_index.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8d546",
   "metadata": {},
   "source": [
    "## Βήμα 4α - Μηχανή αναζήτησης: Επεξεργασία ερωτήματος\n",
    "\n",
    "Για να υλοποιήσουμε τον λογικό επεξεργαστή ερωτημάτων που χειρίζεται Boolean ερωτήματα, αρχικά πρέπει να αναλύσουμε το ερώτημα του χρήστη. Αυτό το κατάφερνουμε κατασκεύζοντας μια συνάρτηση που αναλύει το ερώτημα σε tokens και στη συνέχεια ελέγχουμε σε ποια έγργαφα υπάρχουν τα tokens αυτά μέσο του ευρετηρίου που δημιουργήσαμε στο προηγούμενο βήμα. Όλα αυτά δημιουργόντας μια διεπαφή που επιτρέπει στο χρήστη να αλληλεπιδρά με τη μηχανή αναζήτησης."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce57d07",
   "metadata": {},
   "source": [
    "Κάνουμε import τις απραίτητες βιβλιοθήκες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc1682a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54136760",
   "metadata": {},
   "source": [
    "Ορίζουμε τη συνάρτηση προεπεξεργασίας των ερωτημάτων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94f67c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Προεπεξεργασία ερωτήματος\n",
    "def preprocess_query(query):\n",
    "\n",
    "    query_tokens = re.findall(r'\\w+', query.lower())\n",
    "    return query_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c736d7",
   "metadata": {},
   "source": [
    "Ορίζουμε τη συνάρτηση αξιολόγισης των ερωτημάτων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df0ce94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αξιολόγηση ερωτήματος\n",
    "def evaluate_query(query, inverted_index):\n",
    "    \n",
    "    tokens = preprocess_query(query)\n",
    "    result_set = set()\n",
    "    current_operator = \"OR\"  # Default operator\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in {\"and\", \"or\", \"not\"}:\n",
    "            current_operator = token.upper()\n",
    "        else:\n",
    "            token_docs = set(inverted_index.get(token, []))\n",
    "            if current_operator == \"OR\":\n",
    "                result_set |= token_docs\n",
    "            elif current_operator == \"AND\":\n",
    "                result_set &= token_docs\n",
    "            elif current_operator == \"NOT\":\n",
    "                result_set -= token_docs\n",
    "\n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb991b9e",
   "metadata": {},
   "source": [
    "Ορίζουμε τη συνάρτηση που υλοποιεί τη διεπαφή με την οποία αλληλεπιδρά ο χρήστης, η οποία φορτώνει τα αρχεία του ευρετηρίου και των εγγράφων απο τα πορηγούμενα ερωτήματα και εμφανίζει στο χρήστη τα αποτελέσματα της αναζήτησης σε κατάλληλη μορφή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de4ac348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Διεπαφή χρήστη\n",
    "def search_engine_cli(inverted_index_path, articles_path):\n",
    "    # Φόρτωση δεδομένων\n",
    "    with open(inverted_index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        inverted_index = json.load(f)\n",
    "\n",
    "    with open(articles_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    print(\"Welcome to the Search Engine\")\n",
    "    print(\"Insert your query or type 'exit' for termination:\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nQuery: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Thanks for using the Search Enigne! Terminating...\")\n",
    "            break\n",
    "\n",
    "        matching_docs = evaluate_query(query, inverted_index)\n",
    "\n",
    "        if matching_docs:\n",
    "            print(\"\\nThe suitable files are:\")\n",
    "            for doc_id in matching_docs:\n",
    "                doc_content = articles[doc_id - 1][\"content\"]\n",
    "                print(f\"\\n[File {doc_id}]\")\n",
    "                print(doc_content[:200] + \"...\")  # Εμφανίζει τα πρώτα 200 χαρακτήρες\n",
    "        else:\n",
    "            print(\"\\nNo relatable files found. Please try again:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9bd1e",
   "metadata": {},
   "source": [
    "Τέλος, εκτελούμε τις συναρτήσεις."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64b81520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Search Engine\n",
      "Insert your query or type 'exit' for termination:\n",
      "\n",
      "Query: information and intelligence\n",
      "\n",
      "The suitable files are:\n",
      "\n",
      "[File 1]\n",
      "Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need.  The information need c...\n",
      "\n",
      "[File 2]\n",
      "Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded ...\n",
      "\n",
      "[File 3]\n",
      "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies metho...\n",
      "\n",
      "[File 4]\n",
      "Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an i...\n",
      "\n",
      "[File 5]\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus...\n",
      "\n",
      "[File 6]\n",
      "Deep learning is a subset of machine learning that focuses on utilizing neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration fr...\n",
      "\n",
      "[File 7]\n",
      "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical ...\n",
      "\n",
      "[File 8]\n",
      "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple,...\n",
      "\n",
      "Query: information not intelligence\n",
      "\n",
      "The suitable files are:\n",
      "\n",
      "[File 0]\n",
      "Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrap...\n",
      "\n",
      "[File 9]\n",
      "Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing software. Data with many entries (rows) offer greater statistical power, while data...\n",
      "\n",
      "Query: banana\n",
      "\n",
      "No relatable files found. Please try again:\n",
      "\n",
      "Query: exit\n",
      "Thanks for using the Search Enigne! Terminating...\n"
     ]
    }
   ],
   "source": [
    "# Κύρια εκτέλεση\n",
    "if __name__ == \"__main__\":\n",
    "    # Αρχεία JSON που δημιουργήθηκαν στα προηγούμενα βήματα\n",
    "    inverted_index_path = \"inverted_index.json\"       # Το ανεστραμμένο ευρετήριο\n",
    "    articles_path = \"wikipedia_articles.json\"         # Τα άρθρα της Wikipedia\n",
    "\n",
    "    # Εκτέλεση της διεπαφής\n",
    "    search_engine_cli(inverted_index_path, articles_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af4c0a",
   "metadata": {},
   "source": [
    "Παρατηρούμε ότι με τη δίνοντας ως ερώτημα το \"information and intelligence\", η μηχανή μας επιστρέφει όλα τα άρθα που περιλαμβάνουν και τους δύο όρους, ενώ όταν δίνουμε το ερώτημα \"information not intelligence\", μας επιστρέφει τα άρθρα που περιέχονυ μόνο τον όρο \"information\" και όχι αυτά που περιέχονυ τον όρο \"intelligence\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b857828",
   "metadata": {},
   "source": [
    "## Βήμα 4β - Μηχανή Αναζήτησης: Κατάταξη αποτελεσμάτων\n",
    "\n",
    "Επεκτείνοντας τον κώδικα του ερωτήματος 4. α), ενσωματώνουμε στη μηχανή αναζήτησης τις λειτουργίες κατάταξης των αποτελεσμάτων με χρήση των τεχνικών TF-IDF και ΒΜ25, καθώς επίσης και την εμφάνιση των αποτελεσμάτων αυτών των λειτουργειών σε φιλική προς το χρήστη μορφή. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393c725",
   "metadata": {},
   "source": [
    "Αρχικά κάνουμε import τις απαράιτητες βιβλιοθήκες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664899b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skleaρn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196059a",
   "metadata": {},
   "source": [
    "Υλοποιούμε μια συνάρτηση για κάθε είδος λειτουργίας. Ξεκινάμε με την λειτουργία της Boolean αναζήτησης η οποία βασίζεται στην συνάρτηση του προηούμενου ερωτήματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "820b4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Αναζήτηση\n",
    "def boolean_search(query, inverted_index, documents):\n",
    "    \n",
    "    terms = query.split()\n",
    "    result_set = set(range(len(documents)))  # Ξεκινάμε με όλα τα έγγραφα\n",
    "    operation = \"OR\"  # Default operator\n",
    "    current_set = set()\n",
    "\n",
    "    for term in terms:\n",
    "        if term.upper() in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            operation = term.upper()\n",
    "        else:\n",
    "            matching_docs = set(inverted_index.get(term, []))\n",
    "            if operation == \"AND\":\n",
    "                current_set = current_set & matching_docs if current_set else matching_docs\n",
    "            elif operation == \"OR\":\n",
    "                current_set = current_set | matching_docs\n",
    "            elif operation == \"NOT\":\n",
    "                current_set = current_set - matching_docs\n",
    "    return current_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4c6a8",
   "metadata": {},
   "source": [
    "Μετά υλοποιούμε την συνάρτηση της TF-IDF κατάταξης για την οποία χρησιμοποιούμε τον έτοιμο αλγόριθμο που την εφαρμόζει."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79fa0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Κατάταξη (Vector Space Model)\n",
    "def tfidf_ranking(query, documents):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_vectors = vectorizer.fit_transform(documents)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    return sorted(enumerate(scores, start=0), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614b202",
   "metadata": {},
   "source": [
    "Υλοποιούμε και την συνάρτηση της ΒΜ25 κατάταξης για την οποία χρησιμπούμε τον έτοιμο αλγόριθμο που την εφαρμόζει."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3be882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Κατάταξη\n",
    "def bm25_ranking(query, documents):\n",
    "    \n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    query_tokens = query.split()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    return sorted(enumerate(scores, start=0), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde022bf",
   "metadata": {},
   "source": [
    "Τέλος, υλοποιούμε την διεπαφή με την οποία θα αλληλεπιδρά ο χρήστης και του δίνει την δυνατότητα επιλογής λειτουργίας και εμφανίζει τα αποτελέσματα. Και εκτελούμε το πρόγραμμα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfcc7a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to the Search Engine.\n",
      "1. Boolean Search\n",
      "2. Rank Results (TF-IDF)\n",
      "3. Rank Results (BM25)\n",
      "4. Exit\n",
      "Choose your operation: 4\n",
      "Exiting the Search Engine.\n"
     ]
    }
   ],
   "source": [
    "# Διεπαφή αναζήτησης με επιλογή τεχνικής\n",
    "def search_engine(query, algorithm=\"Boolean\", articles_path=\"wikipedia_articles.json\", index_path=\"inverted_index.json\"):\n",
    "   \n",
    "    # Φόρτωση άρθρων\n",
    "    with open(articles_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "    documents = [article[\"content\"] for article in articles]\n",
    "    titles = [article[\"title\"] for article in articles]\n",
    "\n",
    "    # Φόρτωση ανεστραμμένου ευρετηρίου\n",
    "    with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        inverted_index = json.load(f)\n",
    "\n",
    "    if algorithm == \"Boolean\":\n",
    "        boolean_results = boolean_search(query, inverted_index, documents)\n",
    "        if boolean_results:\n",
    "            print(\"\\nResults of Boolean Search:\")\n",
    "            for doc_id in boolean_results:\n",
    "                print(f\"- {titles[doc_id]}\")\n",
    "        else:\n",
    "            print(\"No relatable files found.\")\n",
    "\n",
    "    elif algorithm == \"TF-IDF\":\n",
    "        ranked_results = tfidf_ranking(query, documents)\n",
    "        print(\"\\nSearch Results (TF-IDF):\")\n",
    "        for doc_id, score in ranked_results[:10]:  # Top 10 αποτελέσματα\n",
    "            if score > 0:\n",
    "                print(f\"- {titles[doc_id]} (Score: {score:.4f})\")\n",
    "\n",
    "    elif algorithm == \"BM25\":\n",
    "        ranked_results = bm25_ranking(query, documents)\n",
    "        print(\"\\nSearch Results (BM25):\")\n",
    "        for doc_id, score in ranked_results[:10]:  # Top 10 αποτελέσματα\n",
    "            if score > 0:\n",
    "                print(f\"- {titles[doc_id]} (Score: {score:.4f})\")\n",
    "\n",
    "    else:\n",
    "        print(\"Unkown algorithm.\")\n",
    "\n",
    "# Εκτέλεση μηχανής αναζήτησης\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        print(\"\\nWelcome to the Search Engine.\")\n",
    "        print(\"1. Boolean Search\")\n",
    "        print(\"2. Rank Results (TF-IDF)\")\n",
    "        print(\"3. Rank Results (BM25)\")\n",
    "        print(\"4. Exit\")\n",
    "        choice = input(\"Choose your operation: \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            query = input(\"Insert the Boolean Query: \")\n",
    "            search_engine(query, algorithm=\"Boolean\")\n",
    "        elif choice == \"2\":\n",
    "            query = input(\"Insert your Query: \")\n",
    "            search_engine(query, algorithm=\"TF-IDF\")\n",
    "        elif choice == \"3\":\n",
    "            query = input(\"Insert your Query: \")\n",
    "            search_engine(query, algorithm=\"BM25\")\n",
    "        elif choice == \"4\":\n",
    "            print(\"Exiting the Search Engine.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Not valid option. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cdea20",
   "metadata": {},
   "source": [
    "Παρακάτω δίνετε ολίκληρως ο κώδικας της μηχανής αναζήτησης για να μπορέσει να τρέξει σωστά στο Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9444e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to the Search Engine.\n",
      "1. Boolean Search\n",
      "2. Rank Results (TF-IDF)\n",
      "3. Rank Results (BM25)\n",
      "4. Exit\n",
      "Choose your operation: 1\n",
      "Insert the Boolean Query: information and intelligence\n",
      "\n",
      "Results of Boolean Search:\n",
      "- Natural language processing\n",
      "- Artificial intelligence\n",
      "- Data mining\n",
      "- Machine learning\n",
      "- Deep learning\n",
      "- Computer vision\n",
      "- Neural network\n",
      "- Big data\n",
      "\n",
      "Welcome to the Search Engine.\n",
      "1. Boolean Search\n",
      "2. Rank Results (TF-IDF)\n",
      "3. Rank Results (BM25)\n",
      "4. Exit\n",
      "Choose your operation: 1\n",
      "Insert the Boolean Query: information not intelligence\n",
      "\n",
      "Results of Boolean Search:\n",
      "- Information retrieval\n",
      "- Data science\n",
      "\n",
      "Welcome to the Search Engine.\n",
      "1. Boolean Search\n",
      "2. Rank Results (TF-IDF)\n",
      "3. Rank Results (BM25)\n",
      "4. Exit\n",
      "Choose your operation: 2\n",
      "Insert your Query: use\n",
      "\n",
      "Search Results (TF-IDF):\n",
      "- Artificial intelligence (Score: 0.0323)\n",
      "- Data mining (Score: 0.0269)\n",
      "- Big data (Score: 0.0260)\n",
      "- Data science (Score: 0.0247)\n",
      "- Machine learning (Score: 0.0173)\n",
      "- Deep learning (Score: 0.0145)\n",
      "- Computer vision (Score: 0.0133)\n",
      "- Information retrieval (Score: 0.0094)\n",
      "- Natural language processing (Score: 0.0082)\n",
      "\n",
      "Welcome to the Search Engine.\n",
      "1. Boolean Search\n",
      "2. Rank Results (TF-IDF)\n",
      "3. Rank Results (BM25)\n",
      "4. Exit\n",
      "Choose your operation: 3\n",
      "Insert your Query: use\n",
      "\n",
      "Search Results (BM25):\n",
      "- Big data (Score: 0.8541)\n",
      "- Artificial intelligence (Score: 0.8488)\n",
      "- Data science (Score: 0.8215)\n",
      "- Machine learning (Score: 0.7957)\n",
      "- Data mining (Score: 0.7591)\n",
      "- Computer vision (Score: 0.7590)\n",
      "- Deep learning (Score: 0.7540)\n",
      "- Information retrieval (Score: 0.6021)\n",
      "- Natural language processing (Score: 0.5805)\n",
      "\n",
      "Welcome to the Search Engine.\n",
      "1. Boolean Search\n",
      "2. Rank Results (TF-IDF)\n",
      "3. Rank Results (BM25)\n",
      "4. Exit\n",
      "Choose your operation: 4\n",
      "Exiting the Search Engine.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "\n",
    "# Boolean Αναζήτηση\n",
    "def boolean_search(query, inverted_index, documents):\n",
    "    \n",
    "    terms = query.split()\n",
    "    result_set = set(range(len(documents)))  # Ξεκινάμε με όλα τα έγγραφα\n",
    "    operation = \"OR\"  # Default operator\n",
    "    current_set = set()\n",
    "\n",
    "    for term in terms:\n",
    "        if term.upper() in [\"AND\", \"OR\", \"NOT\"]:\n",
    "            operation = term.upper()\n",
    "        else:\n",
    "            matching_docs = set(inverted_index.get(term, []))\n",
    "            if operation == \"AND\":\n",
    "                current_set = current_set & matching_docs if current_set else matching_docs\n",
    "            elif operation == \"OR\":\n",
    "                current_set = current_set | matching_docs\n",
    "            elif operation == \"NOT\":\n",
    "                current_set = current_set - matching_docs\n",
    "    return current_set\n",
    "\n",
    "# TF-IDF Κατάταξη (Vector Space Model)\n",
    "def tfidf_ranking(query, documents):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_vectors = vectorizer.fit_transform(documents)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    return sorted(enumerate(scores, start=0), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# BM25 Κατάταξη\n",
    "def bm25_ranking(query, documents):\n",
    "    \n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    query_tokens = query.split()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    return sorted(enumerate(scores, start=0), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Διεπαφή αναζήτησης με επιλογή τεχνικής\n",
    "def search_engine(query, algorithm=\"Boolean\", articles_path=\"wikipedia_articles.json\", index_path=\"inverted_index.json\"):\n",
    "   \n",
    "    # Φόρτωση άρθρων\n",
    "    with open(articles_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "    documents = [article[\"content\"] for article in articles]\n",
    "    titles = [article[\"title\"] for article in articles]\n",
    "\n",
    "    # Φόρτωση ανεστραμμένου ευρετηρίου\n",
    "    with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        inverted_index = json.load(f)\n",
    "\n",
    "    if algorithm == \"Boolean\":\n",
    "        boolean_results = boolean_search(query, inverted_index, documents)\n",
    "        if boolean_results:\n",
    "            print(\"\\nResults of Boolean Search:\")\n",
    "            for doc_id in boolean_results:\n",
    "                print(f\"- {titles[doc_id]}\")\n",
    "        else:\n",
    "            print(\"No relatable files found.\")\n",
    "\n",
    "    elif algorithm == \"TF-IDF\":\n",
    "        ranked_results = tfidf_ranking(query, documents)\n",
    "        print(\"\\nSearch Results (TF-IDF):\")\n",
    "        for doc_id, score in ranked_results[:10]:  # Top 10 αποτελέσματα\n",
    "            if score > 0:\n",
    "                print(f\"- {titles[doc_id]} (Score: {score:.4f})\")\n",
    "\n",
    "    elif algorithm == \"BM25\":\n",
    "        ranked_results = bm25_ranking(query, documents)\n",
    "        print(\"\\nSearch Results (BM25):\")\n",
    "        for doc_id, score in ranked_results[:10]:  # Top 10 αποτελέσματα\n",
    "            if score > 0:\n",
    "                print(f\"- {titles[doc_id]} (Score: {score:.4f})\")\n",
    "\n",
    "    else:\n",
    "        print(\"Unkown algorithm.\")\n",
    "\n",
    "# Εκτέλεση μηχανής αναζήτησης\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        print(\"\\nWelcome to the Search Engine.\")\n",
    "        print(\"1. Boolean Search\")\n",
    "        print(\"2. Rank Results (TF-IDF)\")\n",
    "        print(\"3. Rank Results (BM25)\")\n",
    "        print(\"4. Exit\")\n",
    "        choice = input(\"Choose your operation: \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            query = input(\"Insert the Boolean Query: \")\n",
    "            search_engine(query, algorithm=\"Boolean\")\n",
    "        elif choice == \"2\":\n",
    "            query = input(\"Insert your Query: \")\n",
    "            search_engine(query, algorithm=\"TF-IDF\")\n",
    "        elif choice == \"3\":\n",
    "            query = input(\"Insert your Query: \")\n",
    "            search_engine(query, algorithm=\"BM25\")\n",
    "        elif choice == \"4\":\n",
    "            print(\"Exiting the Search Engine.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Not valid option. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9d08c",
   "metadata": {},
   "source": [
    "Εκτελόυμε μία μία τις λειτουργίες της μηχανής και παρατηρούμε ότι η λειτουργία της Boolean αναζήτησης λειτουργεί όπως και στην προηγούμενη έκδωση της μηαχνής, αλλά αυτή τη φορά εμφανίζονται οι τίτλοι των εγγράφων που αντιστοιχούν στις προϋποθέσεις που ορίζουν τα queries. Για να ελέγξουμε τις λειτουργίες των TF-IDF και BM25 κατατάξεων δίνουμε ώς όρο στη τη λέξη \"use\" και παρατηρούμε ότι η μηαχανή μας επιστρέφει τις κατατάξεις των εγγράφων ανάλογα με το πόσο βρίσκεται ο όρος που δώσαμε στο καθένα και σύμφωνα με τα κρητίρια της κάθε μεθόδου. Για παράδειγμα, βλέπουμε ότι ο όρος \"use\" έχει πιο μεγάλο score στο έγγραφο \"artificial intelligence\" και είναι πιο ψηλά στην κατάταξη χρησιμοποιώντας την TF-IDF σε σχέση με την BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299dfe1",
   "metadata": {},
   "source": [
    "## Βήμα 5 - Αξιολόγηση συστήματος\n",
    "\n",
    "Για να υλοποιήσουμε την αξιολόγηση τους συστήματος χρησιμοποιούμε τις συναρτήσεις από πριν που υπολογίζουν τις κατατάξεις και την Boolean αναζήτηση και με τη χρήση μερικών queries αξιολόγησης στα οποία εφαρμόζουμε όλες τις παραπάνω λειτουργίες, υπολογίζουμε τη μέση ακρίβεια, τη μέση ανάκληση, το μέσο F1-score και τη μέση ακρίβεια μέσης θέσης."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cbe64",
   "metadata": {},
   "source": [
    "Ξεκινάμε κάνοντας import τις απραίτητες βιβλιοθήκες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56180c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bfa895",
   "metadata": {},
   "source": [
    "Στη συνέχεια υλοποιούμε μία συνάρτηση για κάθε μέθοδο αξιολόγησης χρησιμοποιώντας τους έτοιμους αλγόριθμους από τις βιβλιοθήκες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3501ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Κατάταξη\n",
    "def tfidf_ranking(query, documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_vectors = vectorizer.fit_transform(documents)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    return sorted(enumerate(scores, start=0), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# BM25 Κατάταξη\n",
    "def bm25_ranking(query, documents):\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    query_tokens = query.split()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    return sorted(enumerate(scores, start=0), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Boolean Αναζήτηση\n",
    "def boolean_search(query, inverted_index):\n",
    "    terms = query.lower().split()\n",
    "    matching_docs = set(inverted_index.get(terms[0], []))\n",
    "    for term in terms[1:]:\n",
    "        if term in inverted_index:\n",
    "            matching_docs = matching_docs.intersection(inverted_index[term])\n",
    "    return sorted(matching_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8f4e1",
   "metadata": {},
   "source": [
    "Φορτώνουμε το inverted index από το βήμα 3, στο οποίο θα ψάξει η μηχανή αναζήτησης και δημιουργούμε μια λίστα με queries με βάση τα οποία θα γίνει η αξιολόγηση του συστήματος."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f72bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Φόρτωση Inverted Index\n",
    "def load_inverted_index(path=\"inverted_index.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Αξιολόγηση\n",
    "test_queries = [\n",
    "    {\"query\": \"machine learning\", \"relevant_docs\": {0, 2}},\n",
    "    {\"query\": \"data science\", \"relevant_docs\": {1, 3}},\n",
    "    {\"query\": \"natural language processing\", \"relevant_docs\": {4}},\n",
    "    {\"query\": \"artificial intelligence\", \"relevant_docs\": {0, 4, 5}},\n",
    "    {\"query\": \"deep learning\", \"relevant_docs\": {2, 5}},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b1cdc",
   "metadata": {},
   "source": [
    "Τέλος δημιουργούμε την κ΄θρια συνάρτηση του προγράμματος ή οποία καλεί της υπόλοιπες και πραγαμτοποιεί την αξιολόγηση του συστήματος και εκτελούμε το πρόγραμμα αξιολόγησης καλώντας την κάθε φορά με μια από τις τρείς διαφορετικές μεθόδους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35590ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Evaluation...\n",
      "\n",
      "\n",
      "Evaluation of Search Engine (Algorithm: TF-IDF):\n",
      "- Average Precision: 0.2039\n",
      "- Average Recall: 0.9333\n",
      "- Average F1-score: 0.3293\n",
      "- Mean Average Precision (MAP): 0.3401\n",
      "\n",
      "Evaluation of Search Engine (Algorithm: BM25):\n",
      "- Average Precision: 0.2039\n",
      "- Average Recall: 0.9333\n",
      "- Average F1-score: 0.3293\n",
      "- Mean Average Precision (MAP): 0.3922\n",
      "\n",
      "Evaluation of Search Engine (Algorithm: Boolean):\n",
      "- Average Precision: 0.2289\n",
      "- Average Recall: 0.8333\n",
      "- Average F1-score: 0.3558\n",
      "- Mean Average Precision (MAP): 0.3600\n"
     ]
    }
   ],
   "source": [
    "def evaluate_search_engine(algorithm, articles_path=\"wikipedia_articles.json\", inverted_index_path=\"inverted_index.json\"):\n",
    "    with open(articles_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    documents = [article[\"content\"] for article in articles]\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1_scores = []\n",
    "    average_precisions = []\n",
    "\n",
    "    if algorithm == \"Boolean\":\n",
    "        inverted_index = load_inverted_index(inverted_index_path)\n",
    "\n",
    "    for test in test_queries:\n",
    "        query = test[\"query\"]\n",
    "        relevant_docs = test[\"relevant_docs\"]\n",
    "\n",
    "        if algorithm == \"Boolean\":\n",
    "            retrieved_docs = boolean_search(query, inverted_index)\n",
    "        elif algorithm == \"TF-IDF\":\n",
    "            ranked_results = tfidf_ranking(query, documents)\n",
    "            retrieved_docs = [doc_id for doc_id, score in ranked_results if score > 0]\n",
    "        elif algorithm == \"BM25\":\n",
    "            ranked_results = bm25_ranking(query, documents)\n",
    "            retrieved_docs = [doc_id for doc_id, score in ranked_results if score > 0]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown algorithm!\")\n",
    "\n",
    "        y_true = [1 if i in relevant_docs else 0 for i in range(len(documents))]\n",
    "        y_pred = [1 if i in retrieved_docs else 0 for i in range(len(documents))]\n",
    "\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1_scores.append(f1)\n",
    "\n",
    "        ap = 0\n",
    "        hits = 0\n",
    "        for rank, doc_id in enumerate(retrieved_docs, start=1):\n",
    "            if doc_id in relevant_docs:\n",
    "                hits += 1\n",
    "                ap += hits / rank\n",
    "        average_precisions.append(ap / len(relevant_docs) if relevant_docs else 0)\n",
    "\n",
    "    mean_precision = np.mean(all_precisions)\n",
    "    mean_recall = np.mean(all_recalls)\n",
    "    mean_f1 = np.mean(all_f1_scores)\n",
    "    mean_ap = np.mean(average_precisions)\n",
    "\n",
    "    print(f\"\\nEvaluation of Search Engine (Algorithm: {algorithm}):\")\n",
    "    print(f\"- Average Precision: {mean_precision:.4f}\")\n",
    "    print(f\"- Average Recall: {mean_recall:.4f}\")\n",
    "    print(f\"- Average F1-score: {mean_f1:.4f}\")\n",
    "    print(f\"- Mean Average Precision (MAP): {mean_ap:.4f}\")\n",
    "\n",
    "# Εκτέλεση Αξιολόγησης\n",
    "print(\"Start of Evaluation...\\n\")\n",
    "evaluate_search_engine(\"TF-IDF\")\n",
    "evaluate_search_engine(\"BM25\")\n",
    "evaluate_search_engine(\"Boolean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219dc9b",
   "metadata": {},
   "source": [
    "Παίρνουμε τα αποτελέσματα τις αξιολόγησης της μηχανής αναζήτησης κάθε μεθόδου και για κάθε χαρακτηριστικό με βάση τα queries που ορίσαμε μέσα στο πρόγραμμα."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
